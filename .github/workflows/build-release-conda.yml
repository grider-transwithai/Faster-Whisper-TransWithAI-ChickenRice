name: Build and Release with Conda

on:
  push:
    tags:
      - 'v*'
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      release_version:
        description: 'Release version (e.g., v1.0.0)'
        required: false
        type: string
      include_chickenrice:
        description: 'Include Chickenrice model in releases'
        required: false
        type: boolean
        default: false

jobs:
  build-windows:
    name: Build Windows - ${{ matrix.build_label }} ${{ matrix.model_variant }}
    runs-on: windows-latest
    defaults:
      run:
        shell: bash -el {0}  # Important for conda activation on Windows
    strategy:
      matrix:
        include:
          # CUDA 11.8 versions
          - accel: "cuda"
            build_label: "CUDA 11.8"
            cuda: "11.8"
            env_file: "environment-cuda118.yml"
            env_name: "faster-whisper-cu118"
            artifact_suffix: "cu118"
            model_variant: "base"
            hf_model: ""
            rocm_family: ""
            ctranslate2_wheel_url: ""
            rocm_sdk_core_url: ""
            rocm_sdk_libraries_url: ""
          - accel: "cuda"
            build_label: "CUDA 11.8"
            cuda: "11.8"
            env_file: "environment-cuda118.yml"
            env_name: "faster-whisper-cu118"
            artifact_suffix: "cu118-chickenrice"
            model_variant: "chickenrice"
            hf_model: "--hf-model chickenrice0721/whisper-large-v2-translate-zh-v0.2-st-ct2"
            rocm_family: ""
            ctranslate2_wheel_url: ""
            rocm_sdk_core_url: ""
            rocm_sdk_libraries_url: ""
          # CUDA 12.2 versions
          - accel: "cuda"
            build_label: "CUDA 12.2"
            cuda: "12.2"
            env_file: "environment-cuda122.yml"
            env_name: "faster-whisper-cu122"
            artifact_suffix: "cu122"
            model_variant: "base"
            hf_model: ""
            rocm_family: ""
            ctranslate2_wheel_url: ""
            rocm_sdk_core_url: ""
            rocm_sdk_libraries_url: ""
          - accel: "cuda"
            build_label: "CUDA 12.2"
            cuda: "12.2"
            env_file: "environment-cuda122.yml"
            env_name: "faster-whisper-cu122"
            artifact_suffix: "cu122-chickenrice"
            model_variant: "chickenrice"
            hf_model: "--hf-model chickenrice0721/whisper-large-v2-translate-zh-v0.2-st-ct2"
            rocm_family: ""
            ctranslate2_wheel_url: ""
            rocm_sdk_core_url: ""
            rocm_sdk_libraries_url: ""
          # CUDA 12.8 versions
          - accel: "cuda"
            build_label: "CUDA 12.8"
            cuda: "12.8"
            env_file: "environment-cuda128.yml"
            env_name: "faster-whisper-cu128"
            artifact_suffix: "cu128"
            model_variant: "base"
            hf_model: ""
            rocm_family: ""
            ctranslate2_wheel_url: ""
            rocm_sdk_core_url: ""
            rocm_sdk_libraries_url: ""
          - accel: "cuda"
            build_label: "CUDA 12.8"
            cuda: "12.8"
            env_file: "environment-cuda128.yml"
            env_name: "faster-whisper-cu128"
            artifact_suffix: "cu128-chickenrice"
            model_variant: "chickenrice"
            hf_model: "--hf-model chickenrice0721/whisper-large-v2-translate-zh-v0.2-st-ct2"

            rocm_family: ""
            ctranslate2_wheel_url: ""
            rocm_sdk_core_url: ""
            rocm_sdk_libraries_url: ""

          # AMD ROCm/HIP (one ZIP per target family)
          - accel: "rocm"
            build_label: "AMD gfx101X-dgpu"
            cuda: "rocm"
            rocm_family: "gfx101X-dgpu"
            env_file: "environment-rocm.yml"
            env_name: "faster-whisper-rocm"
            artifact_suffix: "amd_gfx101x_dgpu"
            model_variant: "base"
            hf_model: ""
            ctranslate2_wheel_url: "https://github.com/TransWithAI/CTranslate2/releases/download/v4.7.2/ctranslate2-4.7.1-cp310-cp310-win_amd64.whl"
            rocm_sdk_core_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx101X-dgpu/rocm_sdk_core-7.12.0a20260224-py3-none-win_amd64.whl"
            rocm_sdk_libraries_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx101X-dgpu/rocm_sdk_libraries_gfx101x_dgpu-7.12.0a20260224-py3-none-win_amd64.whl"
          - accel: "rocm"
            build_label: "AMD gfx101X-dgpu"
            cuda: "rocm"
            rocm_family: "gfx101X-dgpu"
            env_file: "environment-rocm.yml"
            env_name: "faster-whisper-rocm"
            artifact_suffix: "amd_gfx101x_dgpu-chickenrice"
            model_variant: "chickenrice"
            hf_model: "--hf-model chickenrice0721/whisper-large-v2-translate-zh-v0.2-st-ct2"
            ctranslate2_wheel_url: "https://github.com/TransWithAI/CTranslate2/releases/download/v4.7.2/ctranslate2-4.7.1-cp310-cp310-win_amd64.whl"
            rocm_sdk_core_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx101X-dgpu/rocm_sdk_core-7.12.0a20260224-py3-none-win_amd64.whl"
            rocm_sdk_libraries_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx101X-dgpu/rocm_sdk_libraries_gfx101x_dgpu-7.12.0a20260224-py3-none-win_amd64.whl"
          - accel: "rocm"
            build_label: "AMD gfx103X-dgpu"
            cuda: "rocm"
            rocm_family: "gfx103X-dgpu"
            env_file: "environment-rocm.yml"
            env_name: "faster-whisper-rocm"
            artifact_suffix: "amd_gfx103x_dgpu"
            model_variant: "base"
            hf_model: ""
            ctranslate2_wheel_url: "https://github.com/TransWithAI/CTranslate2/releases/download/v4.7.2/ctranslate2-4.7.1-cp310-cp310-win_amd64.whl"
            rocm_sdk_core_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx103X-dgpu/rocm_sdk_core-7.12.0a20260224-py3-none-win_amd64.whl"
            rocm_sdk_libraries_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx103X-dgpu/rocm_sdk_libraries_gfx103x_dgpu-7.12.0a20260224-py3-none-win_amd64.whl"
          - accel: "rocm"
            build_label: "AMD gfx103X-dgpu"
            cuda: "rocm"
            rocm_family: "gfx103X-dgpu"
            env_file: "environment-rocm.yml"
            env_name: "faster-whisper-rocm"
            artifact_suffix: "amd_gfx103x_dgpu-chickenrice"
            model_variant: "chickenrice"
            hf_model: "--hf-model chickenrice0721/whisper-large-v2-translate-zh-v0.2-st-ct2"
            ctranslate2_wheel_url: "https://github.com/TransWithAI/CTranslate2/releases/download/v4.7.2/ctranslate2-4.7.1-cp310-cp310-win_amd64.whl"
            rocm_sdk_core_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx103X-dgpu/rocm_sdk_core-7.12.0a20260224-py3-none-win_amd64.whl"
            rocm_sdk_libraries_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx103X-dgpu/rocm_sdk_libraries_gfx103x_dgpu-7.12.0a20260224-py3-none-win_amd64.whl"
          - accel: "rocm"
            build_label: "AMD gfx110X-all"
            cuda: "rocm"
            rocm_family: "gfx110X-all"
            env_file: "environment-rocm.yml"
            env_name: "faster-whisper-rocm"
            artifact_suffix: "amd_gfx110x_all"
            model_variant: "base"
            hf_model: ""
            ctranslate2_wheel_url: "https://github.com/TransWithAI/CTranslate2/releases/download/v4.7.2/ctranslate2-4.7.1-cp310-cp310-win_amd64.whl"
            rocm_sdk_core_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx110X-all/rocm_sdk_core-7.12.0a20260224-py3-none-win_amd64.whl"
            rocm_sdk_libraries_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx110X-all/rocm_sdk_libraries_gfx110x_all-7.12.0a20260224-py3-none-win_amd64.whl"
          - accel: "rocm"
            build_label: "AMD gfx110X-all"
            cuda: "rocm"
            rocm_family: "gfx110X-all"
            env_file: "environment-rocm.yml"
            env_name: "faster-whisper-rocm"
            artifact_suffix: "amd_gfx110x_all-chickenrice"
            model_variant: "chickenrice"
            hf_model: "--hf-model chickenrice0721/whisper-large-v2-translate-zh-v0.2-st-ct2"
            ctranslate2_wheel_url: "https://github.com/TransWithAI/CTranslate2/releases/download/v4.7.2/ctranslate2-4.7.1-cp310-cp310-win_amd64.whl"
            rocm_sdk_core_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx110X-all/rocm_sdk_core-7.12.0a20260224-py3-none-win_amd64.whl"
            rocm_sdk_libraries_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx110X-all/rocm_sdk_libraries_gfx110x_all-7.12.0a20260224-py3-none-win_amd64.whl"
          - accel: "rocm"
            build_label: "AMD gfx120X-all"
            cuda: "rocm"
            rocm_family: "gfx120X-all"
            env_file: "environment-rocm.yml"
            env_name: "faster-whisper-rocm"
            artifact_suffix: "amd_gfx120x_all"
            model_variant: "base"
            hf_model: ""
            ctranslate2_wheel_url: "https://github.com/TransWithAI/CTranslate2/releases/download/v4.7.2/ctranslate2-4.7.1-cp310-cp310-win_amd64.whl"
            rocm_sdk_core_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx120X-all/rocm_sdk_core-7.12.0a20260224-py3-none-win_amd64.whl"
            rocm_sdk_libraries_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx120X-all/rocm_sdk_libraries_gfx120x_all-7.12.0a20260224-py3-none-win_amd64.whl"
          - accel: "rocm"
            build_label: "AMD gfx120X-all"
            cuda: "rocm"
            rocm_family: "gfx120X-all"
            env_file: "environment-rocm.yml"
            env_name: "faster-whisper-rocm"
            artifact_suffix: "amd_gfx120x_all-chickenrice"
            model_variant: "chickenrice"
            hf_model: "--hf-model chickenrice0721/whisper-large-v2-translate-zh-v0.2-st-ct2"
            ctranslate2_wheel_url: "https://github.com/TransWithAI/CTranslate2/releases/download/v4.7.2/ctranslate2-4.7.1-cp310-cp310-win_amd64.whl"
            rocm_sdk_core_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx120X-all/rocm_sdk_core-7.12.0a20260224-py3-none-win_amd64.whl"
            rocm_sdk_libraries_url: "https://d2awnip2yjpvqn.cloudfront.net/v2/gfx120X-all/rocm_sdk_libraries_gfx120x_all-7.12.0a20260224-py3-none-win_amd64.whl"

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure stdout buffering
      run: |
        # Enable line buffering for better performance with large outputs
        echo "Configuring buffering for improved CI performance..."
        echo "PYTHONUNBUFFERED=1" >> $GITHUB_ENV
        # For shell commands, we'll use stdbuf where needed
        echo "Buffering configuration complete."

    - name: Cache conda packages
      uses: actions/cache@v4
      id: conda-cache
      env:
        CACHE_NUMBER: 1  # Increment to invalidate cache
      with:
        path: |
          ~/conda_pkgs_dir
        key: ${{ runner.os }}-conda-pkgs-${{ matrix.env_name }}-${{ matrix.cuda }}-${{ env.CACHE_NUMBER }}-${{ hashFiles(matrix.env_file) }}
        restore-keys: |
          ${{ runner.os }}-conda-pkgs-${{ matrix.env_name }}-${{ matrix.cuda }}-${{ env.CACHE_NUMBER }}-
          ${{ runner.os }}-conda-pkgs-${{ matrix.env_name }}-${{ matrix.cuda }}-


    - name: Setup Miniforge
      uses: conda-incubator/setup-miniconda@v3
      with:
        miniforge-version: latest
        auto-update-conda: true
        environment-file: ${{ matrix.env_file }}
        activate-environment: ${{ matrix.env_name }}
        show-channel-urls: true
        use-only-tar-bz2: true
        use-mamba: true  # Use mamba for faster dependency resolution
        # Add conda-pkgs-dir to use cached packages
        pkgs-dirs: ~/conda_pkgs_dir
        python-version: "3.10"

    - name: Install ROCm SDK wheels (AMD)
      if: matrix.accel == 'rocm'
      run: |
        echo "Installing ROCm runtime wheels for ${{ matrix.rocm_family }}..."
        python -m pip install "${{ matrix.rocm_sdk_core_url }}" "${{ matrix.rocm_sdk_libraries_url }}"

    - name: Install HIP-enabled CTranslate2 wheel (AMD)
      if: matrix.accel == 'rocm'
      run: |
        echo "Installing HIP-enabled ctranslate2 wheel from: ${{ matrix.ctranslate2_wheel_url }}"
        python -m pip install --force-reinstall "${{ matrix.ctranslate2_wheel_url }}"
        python -c "import importlib.metadata as md; print(f'CTranslate2 version: {md.version(\"ctranslate2\")}')"

    - name: Force reinstall ctranslate2 for CUDA 11.8
      if: matrix.accel == 'cuda' && matrix.cuda == '11.8'
      run: |
        echo "Force reinstalling ctranslate2==3.24.0 for CUDA 11.8 compatibility..."
        pip install --force-reinstall --no-deps ctranslate2==3.24.0 numpy==1.26.4
        echo "ctranslate2 reinstalled successfully"
        python -c "import importlib.metadata as md; print(f'CTranslate2 version: {md.version(\"ctranslate2\")}')"

    - name: Fix onnxruntime CPU/GPU conflict
      if: matrix.accel == 'cuda'
      run: |
        echo "Removing onnxruntime CPU version to avoid conflicts..."
        pip uninstall onnxruntime -y || true
        echo ""
        echo "Installing appropriate onnxruntime-gpu version for CUDA ${{ matrix.cuda }}..."
        if [ "${{ matrix.cuda }}" = "11.8" ]; then
          echo "Installing onnxruntime-gpu==1.18.0 for CUDA 11.8..."
          pip install onnxruntime-gpu==1.18.0
        elif [ "${{ matrix.cuda }}" = "12.2" ] || [ "${{ matrix.cuda }}" = "12.8" ]; then
          echo "Installing onnxruntime-gpu==1.20.2 for CUDA ${{ matrix.cuda }}..."
          pip install onnxruntime-gpu==1.20.2
        else
          echo "Installing onnxruntime-gpu>=1.17.0 for CUDA ${{ matrix.cuda }}..."
          pip install "onnxruntime-gpu>=1.17.0"
        fi
        echo ""
        echo "Verifying onnxruntime-gpu installation..."
        python -c "import onnxruntime as ort; print(f'ONNX Runtime version: {ort.__version__}'); print(f'Available providers: {ort.get_available_providers()}')" || echo "Note: GPU providers won't show on GitHub runners (no GPU)"

    - name: Apply batch transcribe patch to faster-whisper
      run: |
        echo "Applying batch transcribe patch to faster-whisper package..."
        # Find the faster-whisper package installation directory
        FASTER_WHISPER_PATH=$(python -c "import importlib.util, os; spec = importlib.util.find_spec('faster_whisper'); print(os.path.dirname(spec.origin))")
        echo "faster-whisper package location: $FASTER_WHISPER_PATH"

        # Verify the transcribe.py file exists
        if [ -f "$FASTER_WHISPER_PATH/transcribe.py" ]; then
          echo "Found transcribe.py at: $FASTER_WHISPER_PATH/transcribe.py"

          # Apply the batch transcribe patch
          echo "Applying batch-transcribe.patch..."
          patch -p1 -d "$FASTER_WHISPER_PATH/.." < patches/batch-transcribe.patch

          # Verify patch was applied
          if [ $? -eq 0 ]; then
            echo "Batch transcribe patch applied successfully!"

            # Display the patched sections for verification
            echo ""
            echo "Verifying batch transcribe patch was applied correctly..."
            echo "Checking for max_initial_timestamp_index calculation:"
            grep -A 2 -B 2 "max_initial_timestamp_index = int" "$FASTER_WHISPER_PATH/transcribe.py" || echo "Pattern not found"

            echo ""
            echo "Checking for without_timestamps default value:"
            grep "without_timestamps: bool = False" "$FASTER_WHISPER_PATH/transcribe.py" || echo "Pattern not found"

            echo ""
            echo "Checking for max_initial_timestamp parameter pass-through:"
            grep "max_initial_timestamp=max_initial_timestamp" "$FASTER_WHISPER_PATH/transcribe.py" || echo "Pattern not found"
          else
            echo "WARNING: Batch transcribe patch failed to apply, but continuing build..."
            echo "This might be because the patch is already applied or the file has changed."
          fi
        else
          echo "ERROR: Could not find transcribe.py at expected location!"
          echo "Build will continue without batch transcribe patch."
        fi

    - name: Report conda cache status
      run: |
        echo "Conda packages cache hit: ${{ steps.conda-cache.outputs.cache-hit }}"
        if [ "${{ steps.conda-cache.outputs.cache-hit }}" = "true" ]; then
          echo "Package cache was restored, installation should be faster"
        else
          echo "Package cache miss, downloading packages"
        fi
        echo ""
        echo "Conda environment location:"
        conda info --envs

    - name: Cache HuggingFace and whisper-base models
      uses: actions/cache@v4
      with:
        # Cache HuggingFace model subdirectories and whisper-base, not VAD models
        # The chickenrice model goes into models/whisper-large-v2-translate-zh-v0.2-st-ct2/
        # The whisper-base goes into models/whisper-base/
        path: |
          models/*/
          !models/*.onnx
          !models/*.json
        key: hf-model-${{ matrix.model_variant }}-${{ hashFiles('download_models.py') }}-${{ matrix.hf_model }}-whisper-base
        restore-keys: |
          hf-model-${{ matrix.model_variant }}-${{ hashFiles('download_models.py') }}-${{ matrix.hf_model }}-
          hf-model-${{ matrix.model_variant }}-${{ hashFiles('download_models.py') }}-

    - name: Display environment info
      run: |
        conda info
        conda list
        python --version
        python -c "import importlib.metadata as md; print(f'CTranslate2 version: {md.version(\"ctranslate2\")}')"
        echo "Note: GPU availability check skipped (no GPU on GitHub runners)"

    - name: Check cached models
      run: |
        echo "Checking for cached models..."
        if [ -d "models" ]; then
          echo "Models directory exists:"
          # Use buffered find instead of ls for better performance
          find models -maxdepth 1 -printf "%M %u %g %s %TY-%Tm-%Td %TH:%TM %p\n" 2>/dev/null | head -20
          echo ""
          echo "Model subdirectories (HuggingFace models and whisper-base, cached):"
          # Pre-calculate all sizes at once
          du -sh models/*/ 2>/dev/null | while read size dir; do
            echo "  - $(basename "$dir"): $size"
          done
          echo ""
          echo "Root model files (VAD models, not cached):"
          find models -maxdepth 1 \( -name "*.onnx" -o -name "*.json" \) -printf "%s %p\n" 2>/dev/null | \
            awk '{size=$1; $1=""; printf "  %s (%s)\n", $0, size}' || echo "  No VAD model files yet"
        else
          echo "Models directory does not exist yet"
        fi

    - name: Download models
      run: |
        python download_models.py ${{ matrix.hf_model }}
      continue-on-error: false

    - name: Verify downloaded models
      run: |
        echo "Model files after download:"
        echo ""
        echo "VAD models (not cached, always re-downloaded):"
        find models -maxdepth 1 \( -name "*.onnx" -o -name "*.json" \) -printf "  %p (%s bytes)\n" 2>/dev/null || echo "  No VAD model files found"
        echo ""
        echo "Whisper-base feature extractor files:"
        if [ -d "models/whisper-base" ]; then
          find models/whisper-base -type f -printf "  %p (%s bytes)\n" 2>/dev/null | head -10
        else
          echo "  Not yet downloaded"
        fi
        echo ""
        if [ "${{ matrix.hf_model }}" != "" ]; then
          echo "HuggingFace models (cached):"
          # Pre-calculate all directory sizes
          du -sh models/*/ 2>/dev/null > /tmp/model_sizes.txt
          for dir in models/*/; do
            if [ -d "$dir" ]; then
              echo "  Directory: $(basename "$dir")"
              find "$dir" -maxdepth 1 -type f -printf "    %f (%s bytes)\n" 2>/dev/null | head -10
              size=$(grep "$dir" /tmp/model_sizes.txt | cut -f1)
              echo "  Total size: $size"
              echo ""
            fi
          done
        else
          echo "No HuggingFace models (base variant)"
        fi


    - name: Build with PyInstaller
      run: |
        echo "Using conda environment: $CONDA_DEFAULT_ENV"
        echo "Python path: $(which python)"
        echo "PyInstaller version:"
        python -m pip show pyinstaller
        export PYTHONPATH="${PYTHONPATH}:${PWD}/src"

        python build_windows.py

    - name: Copy models to distribution
      run: |
        echo "Copying models to distribution directory..."
        if [ -d "models" ]; then
          echo "Found models directory"
          echo "Contents of models directory:"
          find models -maxdepth 1 -printf "%M %u %g %s %TY-%Tm-%Td %TH:%TM %p\n" 2>/dev/null
          echo ""

          # Create models directory in dist
          mkdir -p dist/faster_whisper_transwithai_chickenrice/models

          # Copy VAD model files (always included)
          echo "Copying VAD models..."
          cp models/*.onnx dist/faster_whisper_transwithai_chickenrice/models/ 2>/dev/null || true
          cp models/*.json dist/faster_whisper_transwithai_chickenrice/models/ 2>/dev/null || true

          # Copy whisper-base for feature extractor (always included for offline usage)
          echo "Copying whisper-base for feature extractor..."
          if [ -d "models/whisper-base" ]; then
            echo "  Found whisper-base directory, copying..."
            cp -r models/whisper-base dist/faster_whisper_transwithai_chickenrice/models/ 2>/dev/null || true
            echo "  Whisper-base copied for offline feature extractor support"
          else
            echo "  WARNING: whisper-base directory not found"
          fi

          # Copy HuggingFace models if this is a chickenrice variant
          if [ "${{ matrix.model_variant }}" = "chickenrice" ]; then
            echo "Copying Chickenrice model..."
            for dir in models/*/; do
              if [ -d "$dir" ]; then
                model_name=$(basename "$dir")
                # Skip whisper-base as we already copied it
                if [ "$model_name" != "whisper-base" ]; then
                  echo "  Copying model contents from: $model_name"
                  # Copy the contents of the model directory, not the directory itself
                  cp -r "$dir"* dist/faster_whisper_transwithai_chickenrice/models/ 2>/dev/null || true
                  # Also copy hidden files if any exist
                  cp -r "$dir".* dist/faster_whisper_transwithai_chickenrice/models/ 2>/dev/null || true
                fi
              fi
            done
          fi

          echo ""
          echo "Models in distribution:"
          find dist/faster_whisper_transwithai_chickenrice/models -maxdepth 1 -printf "%M %u %g %s %TY-%Tm-%Td %TH:%TM %p\n" 2>/dev/null
          echo ""
          echo "Total distribution size:"
          du -sh dist/faster_whisper_transwithai_chickenrice/
        else
          echo "WARNING: Models directory not found!"
        fi

    - name: Copy batch files, configuration, and documentation
      run: |
        echo "Copying batch files, configuration, and documentation to distribution..."

        # Copy usage instructions
        if [ -f "ä½¿ç”¨è¯´æ˜Ž.txt" ]; then
          cp "ä½¿ç”¨è¯´æ˜Ž.txt" dist/faster_whisper_transwithai_chickenrice/
          echo "Copied: ä½¿ç”¨è¯´æ˜Ž.txt"
        fi

        # Copy release notes
        if [ -f "RELEASE_NOTES_CN.md" ]; then
          cp "RELEASE_NOTES_CN.md" dist/faster_whisper_transwithai_chickenrice/
          echo "Copied: RELEASE_NOTES_CN.md"
        fi

        # Copy generation config to root directory (for easy user editing)
        if [ -f "generation_config.json5" ]; then
          cp "generation_config.json5" dist/faster_whisper_transwithai_chickenrice/
          echo "Copied: generation_config.json5 to root directory"
        fi

        # Copy all batch files
        for bat_file in *.bat; do
          if [ -f "$bat_file" ]; then
            # Skip any build-related batch files
            if [[ "$bat_file" != *"build"* ]] && [[ "$bat_file" == "è¿è¡Œ"* ]]; then
              cp "$bat_file" dist/faster_whisper_transwithai_chickenrice/
              echo "Copied: $bat_file"
            fi
          fi
        done

        echo ""
        echo "Distribution contents:"
        find dist/faster_whisper_transwithai_chickenrice -maxdepth 1 \( -name "*.bat" -o -name "*.txt" -o -name "*.md" -o -name "*.json5" \) -printf "%M %u %g %s %TY-%Tm-%Td %TH:%TM %p\n" 2>/dev/null || echo "No batch/text/config files found"

    - name: Test executable (CPU mode)
      shell: bash -el {0}
      run: |
        cd dist/faster_whisper_transwithai_chickenrice

        echo "Testing infer.exe --help..."
        INFER_OUTPUT=$(./infer.exe --help 2>&1)
        echo "$INFER_OUTPUT"

        # Verify infer.exe output contains expected strings
        if echo "$INFER_OUTPUT" | grep -q "Whisper transcription with custom VAD injection"; then
          echo "âœ“ Found expected description in infer.exe output"
        else
          echo "âœ— Missing expected description in infer.exe output"
          exit 1
        fi

        if echo "$INFER_OUTPUT" | grep -q "positional arguments:"; then
          echo "âœ“ Found positional arguments section"
        else
          echo "âœ— Missing positional arguments section"
          exit 1
        fi

        if echo "$INFER_OUTPUT" | grep -q -- "--model_name_or_path"; then
          echo "âœ“ Found --model_name_or_path argument"
        else
          echo "âœ— Missing --model_name_or_path argument"
          exit 1
        fi

        echo ""
        echo "Testing modal_infer.exe --help..."
        MODAL_OUTPUT=$(./modal_infer.exe --help 2>&1)
        echo "$MODAL_OUTPUT"

        # Verify modal_infer.exe output contains expected strings
        if echo "$MODAL_OUTPUT" | grep -qi "modal"; then
          echo "âœ“ Found 'modal' in modal_infer.exe output"
        else
          echo "âœ— Missing 'modal' in modal_infer.exe output"
          exit 1
        fi

        echo ""
        echo "All executable tests passed!"

    - name: Upload artifact
      uses: actions/upload-artifact@v4
      with:
        name: faster_whisper_transwithai_windows_${{ matrix.artifact_suffix }}
        path: dist/faster_whisper_transwithai_chickenrice/
        retention-days: 30

    - name: List artifact directory structure
      run: |
        echo "========================================================================"
        echo "ðŸ“¦ ARTIFACT DIRECTORY STRUCTURE"
        echo "========================================================================"
        echo "Build variant: ${{ matrix.artifact_suffix }}"
        echo "CUDA version: ${{ matrix.cuda }}"
        echo "Model variant: ${{ matrix.model_variant }}"
        echo "------------------------------------------------------------------------"

        # Simple directory tree (depth limited to 3)
        echo "Directory structure:"
        find dist/faster_whisper_transwithai_chickenrice -type d -maxdepth 3 | \
          sed 's|[^/]*/|  |g' | \
          sed 's|^  |dist/|'

        echo ""
        echo "Total artifact size: $(du -sh dist/faster_whisper_transwithai_chickenrice/ | cut -f1)"
        echo "========================================================================"

  # Create the initial GitHub release
  create-release:
    name: Create GitHub Release
    needs: [build-windows]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write
    outputs:
      release_created: ${{ steps.create.outputs.release_created }}

    steps:
    - name: Checkout code for release notes
      uses: actions/checkout@v4
      with:
        sparse-checkout: |
          RELEASE_NOTES_CN.md
        sparse-checkout-cone-mode: false

    - name: Read release body
      id: read_body
      run: |
        if [ -f "RELEASE_NOTES_CN.md" ]; then
          echo 'body<<EOF' >> $GITHUB_OUTPUT
          cat RELEASE_NOTES_CN.md >> $GITHUB_OUTPUT
          echo 'EOF' >> $GITHUB_OUTPUT
        else
          echo 'body=Release created by GitHub Actions' >> $GITHUB_OUTPUT
        fi

    - name: Create empty placeholder file for initial release
      run: |
        echo "This release contains large binary files. Files are being uploaded..." > placeholder.txt

    - name: Create Release with placeholder
      id: create
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        name: ${{ github.ref_name }}
        body: ${{ steps.read_body.outputs.body }}
        draft: false
        prerelease: false
        files: placeholder.txt
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set output
      run: echo "release_created=true" >> $GITHUB_OUTPUT

  # Parallel upload jobs - each handles one artifact
  upload-cu118:
    name: Upload CUDA 11.8 Base
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_cu118
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for CUDA 11.8 base variant..."
        # Using compression level 5 for faster builds (was level 9)
        # Level 5 provides good balance between speed and compression ratio
        zip -5 -r -q ../faster_whisper_transwithai_windows_cu118.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_cu118.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_cu118.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-cu118-chickenrice:
    name: Upload CUDA 11.8 Chickenrice
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_cu118-chickenrice
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for CUDA 11.8 chickenrice variant..."
        # Using compression level 5 for faster builds (was level 9)
        zip -5 -r -q ../faster_whisper_transwithai_windows_cu118-chickenrice.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_cu118-chickenrice.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_cu118-chickenrice.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-cu122:
    name: Upload CUDA 12.2 Base
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_cu122
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for CUDA 12.2 base variant..."
        # Using compression level 5 for faster builds (was level 9)
        zip -5 -r -q ../faster_whisper_transwithai_windows_cu122.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_cu122.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_cu122.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-cu122-chickenrice:
    name: Upload CUDA 12.2 Chickenrice
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_cu122-chickenrice
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for CUDA 12.2 chickenrice variant..."
        # Using compression level 5 for faster builds (was level 9)
        zip -5 -r -q ../faster_whisper_transwithai_windows_cu122-chickenrice.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_cu122-chickenrice.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_cu122-chickenrice.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-cu128:
    name: Upload CUDA 12.8 Base
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_cu128
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for CUDA 12.8 base variant..."
        # Using compression level 5 for faster builds (was level 9)
        zip -5 -r -q ../faster_whisper_transwithai_windows_cu128.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_cu128.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_cu128.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-cu128-chickenrice:
    name: Upload CUDA 12.8 Chickenrice
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_cu128-chickenrice
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for CUDA 12.8 chickenrice variant..."
        # Using compression level 5 for faster builds (was level 9)
        zip -5 -r -q ../faster_whisper_transwithai_windows_cu128-chickenrice.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_cu128-chickenrice.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_cu128-chickenrice.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-amd-gfx101x:
    name: Upload AMD gfx101X Base
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_amd_gfx101x_dgpu
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for AMD gfx101X base variant..."
        zip -5 -r -q ../faster_whisper_transwithai_windows_amd_gfx101x_dgpu.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_amd_gfx101x_dgpu.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_amd_gfx101x_dgpu.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-amd-gfx101x-chickenrice:
    name: Upload AMD gfx101X Chickenrice
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_amd_gfx101x_dgpu-chickenrice
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for AMD gfx101X chickenrice variant..."
        zip -5 -r -q ../faster_whisper_transwithai_windows_amd_gfx101x_dgpu-chickenrice.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_amd_gfx101x_dgpu-chickenrice.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_amd_gfx101x_dgpu-chickenrice.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-amd-gfx103x:
    name: Upload AMD gfx103X Base
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_amd_gfx103x_dgpu
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for AMD gfx103X base variant..."
        zip -5 -r -q ../faster_whisper_transwithai_windows_amd_gfx103x_dgpu.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_amd_gfx103x_dgpu.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_amd_gfx103x_dgpu.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-amd-gfx103x-chickenrice:
    name: Upload AMD gfx103X Chickenrice
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_amd_gfx103x_dgpu-chickenrice
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for AMD gfx103X chickenrice variant..."
        zip -5 -r -q ../faster_whisper_transwithai_windows_amd_gfx103x_dgpu-chickenrice.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_amd_gfx103x_dgpu-chickenrice.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_amd_gfx103x_dgpu-chickenrice.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-amd-gfx110x:
    name: Upload AMD gfx110X Base
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_amd_gfx110x_all
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for AMD gfx110X base variant..."
        zip -5 -r -q ../faster_whisper_transwithai_windows_amd_gfx110x_all.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_amd_gfx110x_all.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_amd_gfx110x_all.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-amd-gfx110x-chickenrice:
    name: Upload AMD gfx110X Chickenrice
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_amd_gfx110x_all-chickenrice
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for AMD gfx110X chickenrice variant..."
        zip -5 -r -q ../faster_whisper_transwithai_windows_amd_gfx110x_all-chickenrice.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_amd_gfx110x_all-chickenrice.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_amd_gfx110x_all-chickenrice.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-amd-gfx120x:
    name: Upload AMD gfx120X Base
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_amd_gfx120x_all
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for AMD gfx120X base variant..."
        zip -5 -r -q ../faster_whisper_transwithai_windows_amd_gfx120x_all.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_amd_gfx120x_all.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_amd_gfx120x_all.zip
        token: ${{ secrets.GITHUB_TOKEN }}

  upload-amd-gfx120x-chickenrice:
    name: Upload AMD gfx120X Chickenrice
    needs: [create-release]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write

    steps:
    - name: Download artifact
      uses: actions/download-artifact@v6
      with:
        name: faster_whisper_transwithai_windows_amd_gfx120x_all-chickenrice
        path: artifact/

    - name: Create archive with optimized compression
      run: |
        cd artifact
        echo "Creating archive for AMD gfx120X chickenrice variant..."
        zip -5 -r -q ../faster_whisper_transwithai_windows_amd_gfx120x_all-chickenrice.zip .
        cd ..
        echo "Archive created: $(ls -lh faster_whisper_transwithai_windows_amd_gfx120x_all-chickenrice.zip | awk '{print $5}')"

    - name: Upload to release with large file support
      uses: ading2210/gh-large-releases@v1
      with:
        repository: ${{ github.repository }}
        tag_name: ${{ github.ref }}
        files: faster_whisper_transwithai_windows_amd_gfx120x_all-chickenrice.zip
        token: ${{ secrets.GITHUB_TOKEN }}
